-- Migration: Add FAQs for Chapters 8-14 (Distributed Systems)
-- Remote module IDs: Ch8=10, Ch9=11, Ch10=12, Ch11=13, Ch12=14, Ch13=15, Ch14=16

-- Chapter 8: Introduction and Overview (Distributed Systems) - module_id=10
INSERT INTO faqs (module_id, question, answer, tags) VALUES
(10, 'Why is distributed systems different from concurrent programming?', 'In concurrent systems, processors can use shared memory to exchange information. In distributed systems, each processor has local state and participants communicate only by passing messages. This makes coordination harder and introduces network-related failure modes.', '["fundamentals", "concurrency"]'),
(10, 'What does exactly-once delivery really mean?', 'Exactly-once delivery is about processing, not transport. TCP may retransmit packets multiple times, but deduplicates them so the message is processed once per session. True exactly-once requires common knowledge - all nodes agreeing the message was or was not persisted.', '["delivery-semantics", "messaging"]'),
(10, 'Why cannot we detect process failures reliably in asynchronous systems?', 'Without timing assumptions, there is no way to distinguish between a crashed process and a slow one. A process that does not respond might be dead, overloaded, or just experiencing network delays. This is why FLP proves consensus is impossible in purely asynchronous systems.', '["FLP", "failure-detection"]'),
(10, 'What is the practical implication of the Two Generals Problem?', 'It shows that perfect agreement over unreliable networks is impossible. In practice, we accept probabilistic guarantees: after enough ACKs, we proceed with high confidence. Systems use timeouts and retries rather than waiting for perfect certainty.', '["theory", "agreement"]'),
(10, 'How do circuit breakers help prevent cascading failures?', 'Circuit breakers monitor failures and allow fallback mechanisms. When a service starts failing, the breaker opens to stop sending requests, giving the service time to recover. This prevents one failure from overwhelming dependent services.', '["resilience", "patterns"]'),
(10, 'What is partial synchrony and why does it matter?', 'Partial synchrony means the system exhibits some synchronous properties (bounded delays) but bounds may not be exact and hold only most of the time. It is a realistic model for real systems and allows consensus algorithms like Raft and Paxos to work in practice.', '["timing", "system-models"]');

-- Chapter 9: Failure Detection - module_id=11
INSERT INTO faqs (module_id, question, answer, tags) VALUES
(11, 'Why is failure detection impossible to do perfectly?', 'In asynchronous systems, you cannot distinguish a crashed process from a slow one. Even with timeouts, you face a trade-off: detect failures quickly (risking false positives) or wait longer to confirm (risking delayed detection). No algorithm can be both perfectly accurate and efficient.', '["theory", "fundamentals"]'),
(11, 'When should I use phi-accrual vs simple timeout-based failure detection?', 'Use phi-accrual when network conditions vary and you need adaptive behavior - it automatically adjusts to changing latencies. Use simple timeouts for stable networks with predictable latency where simplicity is valued over adaptability.', '["practical", "comparison"]'),
(11, 'How does SWIM outsourced heartbeats improve accuracy?', 'Instead of relying on one nodes direct observation, SWIM asks multiple random peers to check the suspected node. This detects whether the node is truly failed or just unreachable from one specific node due to a localized network issue.', '["SWIM", "algorithms"]'),
(11, 'What is the benefit of converting individual failures to group failures in FUSE?', 'It guarantees all members learn about the failure through quiescence (absence of responses). No active propagation is needed - failure spreads automatically. Useful when you need all participants to react to any failure, though it may be aggressive for some use cases.', '["FUSE", "algorithms"]');

-- Chapter 10: Leader Election - module_id=12
INSERT INTO faqs (module_id, question, answer, tags) VALUES
(12, 'How do I choose between different leader election algorithms?', 'Consider: network topology (ring algorithm needs ring structure), failure tolerance needs (bully is simple but vulnerable to split brain), message overhead (invitation reduces messages via group merging), and whether you need single or multiple leaders.', '["practical", "algorithms"]'),
(12, 'Why do Multi-Paxos and Raft allow multiple leaders temporarily?', 'For performance - they can proceed with replication without waiting for perfect leader agreement. Safety is maintained because conflicts are detected and resolved: Multi-Paxos uses quorums, Raft uses term numbers. This trades perfect safety for better liveness.', '["consensus", "trade-offs"]'),
(12, 'How can I prevent split brain in leader election?', 'Require a cluster-wide majority (quorum) of votes to become leader. With majority voting, only one leader can be elected even during partitions. This is why consensus algorithms like Raft require majority agreement.', '["split-brain", "quorum"]');

-- Chapter 11: Replication and Consistency - module_id=13
INSERT INTO faqs (module_id, question, answer, tags) VALUES
(13, 'When should I choose eventual consistency over strong consistency?', 'Choose eventual consistency when: availability is critical, brief inconsistency is acceptable, operations are naturally commutative or idempotent, and you can implement conflict resolution. Choose strong consistency when: data integrity is paramount, operations are not idempotent, or users expect immediate visibility of changes.', '["practical", "trade-offs"]'),
(13, 'Why is CAP often misunderstood?', 'CAP is often shown as a triangle where you pick 2 of 3, but partition tolerance is not optional - partitions will happen. CAP consistency means linearizability (not ACID consistency). CAP availability means all non-failed nodes respond (not high availability). It describes behavior during partitions, not normal operation.', '["CAP", "theory"]'),
(13, 'How do CRDTs achieve strong eventual consistency?', 'CRDTs (Conflict-free Replicated Data Types) are data structures designed so that concurrent updates can always be merged deterministically. Examples: G-Counter (grow-only), PN-Counter (increment/decrement), LWW-Register (last-write-wins), OR-Set (observed-remove set). Mathematical properties guarantee convergence.', '["CRDTs", "eventual-consistency"]'),
(13, 'What is the quorum formula for consistency?', 'For N replicas, if W + R > N (where W = write acknowledgments required, R = read acknowledgments required), reads and writes overlap ensuring consistency. Common configurations: W=N, R=1 (fast reads); W=1, R=N (fast writes); W=R=(N+1)/2 (balanced).', '["quorum", "practical"]');

-- Chapter 12: Anti-Entropy and Dissemination - module_id=14
INSERT INTO faqs (module_id, question, answer, tags) VALUES
(14, 'When should I use read repair vs Merkle trees for anti-entropy?', 'Read repair: good for frequently accessed data, catches inconsistencies on the hot path, low overhead for popular data. Merkle trees: good for background reconciliation of cold data, catches inconsistencies in data that is rarely read. Most systems use both together.', '["practical", "anti-entropy"]'),
(14, 'What is the relationship between fanout and convergence in gossip?', 'Higher fanout = faster convergence but more redundancy. Lower fanout = slower convergence but less overhead. For a cluster of N nodes, gossip typically converges in O(log N) rounds. Fanout must be tuned based on cluster size and tolerance for latency vs overhead.', '["gossip", "tuning"]'),
(14, 'Why do systems use multiple anti-entropy mechanisms together?', 'Each mechanism has different strengths: read repair handles hot data, hinted handoff handles write failures, Merkle trees handle cold data, gossip handles metadata. Together they provide defense in depth - if one mechanism misses something, another catches it.', '["architecture", "reliability"]');

-- Chapter 13: Distributed Transactions - module_id=15
INSERT INTO faqs (module_id, question, answer, tags) VALUES
(15, 'Why is 2PC still used despite its blocking problem?', 'Simplicity, low message overhead, and well-understood failure modes. The blocking scenario requires coordinator failure at a specific moment, which is rare. Most systems mitigate by using coordinator replication, backup coordinators, or running 2PC over consensus groups (like Spanner).', '["2PC", "practical"]'),
(15, 'When should I avoid distributed transactions?', 'When possible! Design systems to keep related data on the same partition. Use saga patterns for cross-service transactions. Distributed transactions add latency, reduce availability, and increase complexity. Only use when strong consistency across partitions is truly required.', '["architecture", "trade-offs"]'),
(15, 'How does Calvin avoid the problems of 2PC?', 'By agreeing on transaction order before execution. All nodes execute the same deterministic order, so node failures do not cause aborts - peers have enough information to recreate state. Removes 2PC coordination during execution, moving it to the ordering phase.', '["Calvin", "algorithms"]');

-- Chapter 14: Consensus - module_id=16
INSERT INTO faqs (module_id, question, answer, tags) VALUES
(16, 'Why is Paxos considered difficult to understand?', 'The original paper used Greek parliament metaphor. The algorithm handles many edge cases (competing proposers, failures at various stages, message reordering). Multi-Paxos (practical version) was never formally specified. Raft was created specifically to address this complexity.', '["Paxos", "history"]'),
(16, 'When should I use Raft vs Paxos?', 'In practice, both achieve similar results. Raft is easier to understand and implement correctly, with clearer leader election and log replication. Use Raft for new implementations. Use Paxos variants (like Multi-Paxos) if you need specific optimizations or are extending existing Paxos systems.', '["comparison", "practical"]'),
(16, 'What is the relationship between consensus and atomic broadcast?', 'They are equivalent problems in asynchronous systems with crash failures. Consensus decides on a single value; atomic broadcast delivers an ordered sequence of values. You can implement one using the other: consensus per message gives atomic broadcast; atomic broadcast of proposals gives consensus.', '["theory", "equivalence"]'),
(16, 'Why do consensus algorithms use failure detectors?', 'FLP proves consensus is impossible in purely asynchronous systems. Failure detectors provide timing assumptions (eventually accurate detection) that allow algorithms to guarantee liveness. The algorithm remains safe even with incorrect detections; liveness requires eventual accuracy.', '["FLP", "theory"]');
